[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m Sarah Pan. I work on AI/ML stuff and occasionally write about things I find interesting.\nThis blog is called “rabbitholes” because I tend to get sucked into random technical topics and spend way too much time exploring them.\nFeel free to reach out on Twitter or GitHub."
  },
  {
    "objectID": "posts/titan-llama/index.html",
    "href": "posts/titan-llama/index.html",
    "title": "Titan-LLaMA: Neural Memory Adapters for Continual Learning",
    "section": "",
    "text": "In NLP, we already have a bunch of very strong, off-the-shelf LLMs, but their “memory” cost grows quadratically with sequence length. Most open-source fixes are linear-time hacks—sliding windows, RAG, caching tricks, etc—rather than a true learned state. As we discuss in the background, these approaches keep the model stateless and outsource memory to context or retrieval.\nTitans stood out to us because it is a very unique approach that does the opposite: the state is a learned architecture, not a transient context vector that disappears when the window moves on. We wanted to extend that idea and build modular Titan adapters that augment a pre-trained LLM whose long-range memory we’ve deliberately incapacitated.\nNo one has done this before: combining Titans-style neural memory with a frozen backbone as a drop-in “memory cartridge”. The result is an architecture that (i) generates tokens much faster as generation length grows and (ii) can be fine-tuned by swapping in task-specific memory adapters for downstream domains.\nArchitecturally, this buys us a different scaling regime. Full-attention LLaMA pays a quadratic price in sequence length, so tokens/sec inevitably drop as generations get longer. Once we segment attention, each token only attends within a fixed-size window, so generation cost is tied to segment size rather than total sequence length. With a well-optimized NMM, a segmented-with-memory model should overtake full attention beyond a few segments because the longer you generate, the larger the relative speedup.\n\n\n\nToken throughput vs. generation length. Full-attention LLaMA slows down as sequences grow, while segmented attention keeps throughput roughly flat; our segmented+NMM model should eventually beat full attention once the memory kernels are optimized.\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that in our prototype the throughput curve still lags slightly behind full attention because the NMM kernels are under-optimized and add extra intermediate computation, but that is a systems issue, not a limitation of the design."
  },
  {
    "objectID": "posts/titan-llama/index.html#motivation",
    "href": "posts/titan-llama/index.html#motivation",
    "title": "Titan-LLaMA: Neural Memory Adapters for Continual Learning",
    "section": "",
    "text": "In NLP, we already have a bunch of very strong, off-the-shelf LLMs, but their “memory” cost grows quadratically with sequence length. Most open-source fixes are linear-time hacks—sliding windows, RAG, caching tricks, etc—rather than a true learned state. As we discuss in the background, these approaches keep the model stateless and outsource memory to context or retrieval.\nTitans stood out to us because it is a very unique approach that does the opposite: the state is a learned architecture, not a transient context vector that disappears when the window moves on. We wanted to extend that idea and build modular Titan adapters that augment a pre-trained LLM whose long-range memory we’ve deliberately incapacitated.\nNo one has done this before: combining Titans-style neural memory with a frozen backbone as a drop-in “memory cartridge”. The result is an architecture that (i) generates tokens much faster as generation length grows and (ii) can be fine-tuned by swapping in task-specific memory adapters for downstream domains.\nArchitecturally, this buys us a different scaling regime. Full-attention LLaMA pays a quadratic price in sequence length, so tokens/sec inevitably drop as generations get longer. Once we segment attention, each token only attends within a fixed-size window, so generation cost is tied to segment size rather than total sequence length. With a well-optimized NMM, a segmented-with-memory model should overtake full attention beyond a few segments because the longer you generate, the larger the relative speedup.\n\n\n\nToken throughput vs. generation length. Full-attention LLaMA slows down as sequences grow, while segmented attention keeps throughput roughly flat; our segmented+NMM model should eventually beat full attention once the memory kernels are optimized.\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that in our prototype the throughput curve still lags slightly behind full attention because the NMM kernels are under-optimized and add extra intermediate computation, but that is a systems issue, not a limitation of the design."
  },
  {
    "objectID": "posts/titan-llama/index.html#background",
    "href": "posts/titan-llama/index.html#background",
    "title": "Titan-LLaMA: Neural Memory Adapters for Continual Learning",
    "section": "Background",
    "text": "Background\nPrior work on adapting language models falls into two categories.\nMemory and state. RNNs and LSTMs compress history into a fixed-size hidden vector but forget distant information. SSMs [@gu2021efficiently] revisit this idea by improving stability and efficiency with linear dynamics. Transformers on the other hand store everything explicitly via attention, relying on a KV cache that grows with sequence length. This means every new token attends over all previous ones, which gives precise access to the past but with quadratic cost in the context length.\n\n\n\n\n\n\nTip\n\n\n\nThis illustrates a core tradeoff in ML: recurrent models compress history into a fixed-size state but struggle with very long contexts, while transformers store everything explicitly and become slow and memory-hungry.\n\n\nParameter-efficient adapters. Full fine-tuning is expensive and risks catastrophic forgetting. LoRA [@hu2021lora] instead adds trainable low-rank factors to frozen weight matrices, reducing trainable parameters by orders of magnitude while preserving expressivity. Cartridges [@hewitt2024cartridges] go further, treating the KV cache as an adapter: learned virtual KV vectors are prepended to the context so the frozen model behaves as if it had read a particular corpus. Both keep backbone weights untouched.\n\nTitans: Deep Neural Memory as State\nTitans: deep neural memory as state. Titans [@behrouz2024titans], introduced by Behrouz et al. [Google Research] in 2024, combines transformer-level accuracy with RNN-like speeds by maintaining a deep neural memory alongside attention. Titans introduces three fundamental innovations:\n1. True Test-Time Memory. Instead of just expanding context windows or adding retrieval mechanisms, Titans learns to memorize at test time. The neural long-term memory module learns what to remember while running, gets smarter with every interaction, and maintains context across extended use.\n2. Surprise-Based Memory. Rather than treating every token in the context as equally important, Titans uses a learned “surprise” score to decide what is worth remembering and therefore what actually gets written into its neural memory. We can compare this to how in human psychology, we quickly forget routine events but remember unexpected, surprising, or highly emotional events.\n\n\n\n\n\n\nWarning\n\n\n\nLow surprise: If the model is reading the 6.7960 course page and its memory is expecting yet another “PSET released” announcement, the gradient (surprise) is low so it can safely skip memorizing this in permanent storage.\nHigh surprise: If the same course page suddenly says “All remaining PSETs are cancelled; everyone gets an A! =)” the gradient will be very high, indicating that this is important and should be prioritized for permanent storage.\n\n\nThe model uses this internal error signal (the gradient) as a mathematical equivalent of saying “Pay attention to this!” This allows selective memory updates with only the most important information.\nTitans considers both “momentary surprise” (current input) and “past surprise” (recent context flow) to make sure that relevant subsequent information is captured even if individual tokens aren’t surprising.\n\n\n\nSurprise update rule. Memory Mt = Mt-1 + St, where St mixes momentum from past surprise with the current prediction-error gradient.\n\n\n3. Adaptive Forgetting. To better manage the memory’s limited capacity, Titans adds an adaptive forgetting gate (similar to the gating mechanism in modern RNNs) that scales the previous memory before adding the new surprise update, so the model can softly decay stale information or, in the extreme, wipe the memory entirely.\nTitans factor memory into three pieces:\n\nShort-term memory: localized attention windows over recent tokens\nLong-term memory: a small MLP that maps keys to values, updated online via the surprise loss\nPersistent memory: task-level parameters independent of the input sequence\n\n\n\n\n\n\n\nNote\n\n\n\nNote that Ali Behrouz (Titans author) mentions on the Cognitive Revolution podcast [@behrouz2025podcast] that persistent memory is mostly there for a human-memory analogy and isn’t really needed in practice, so we ignore it in our implementation and just add the short and long-term memory pieces.\n\n\n\n\n\nTitans MAC architecture.\n\n\nLegend: - Top band: long-term contextual memory (Neural Memory) - Middle band: short-term sequence + attention (Core)\n- Bottom band: persistent task-specific parameters (Persistent Memory)\nNeural memory stores a compressed summary of past tokens that is retrieved, updated, and injected back into attention over the current sequence.\nIn summary, memory is not a vector or cache but a trainable module whose weights are the state—the model learns and adapts dynamically as new data arrives."
  },
  {
    "objectID": "posts/titan-llama/index.html#methods",
    "href": "posts/titan-llama/index.html#methods",
    "title": "Titan-LLaMA: Neural Memory Adapters for Continual Learning",
    "section": "Methods",
    "text": "Methods\nOur perspective: We combine these views by attaching Titan-style memory to a frozen LLaMA backbone and treating it as a cartridge-style adapter. The backbone stays fixed; only the Titan memory continues learning.\n\n\n\nThe NMM (green) injects cross-segment attention entries, allowing information retrieval across segment boundaries without full-sequence attention.\n\n\nThe next step of the process is to augment the localized attention calculation with “long-term memory” that is retrieved from the NMM. The NMM will try to recall what happened at the previous local attention window.\n\n\n\nAs chunks c1–c5 progress, the NMM is queried then updated based on prediction error, persisting information across the full sequence.\n\n\nIt might be helpful to “flip” the time axis when thinking about the fast update process. If we imagine each localized attention segment to be its own element in a sequence, this process closely resembles that of an LSTM or RNN. However, instead of learning weights that modulate previous hidden cell states, the hidden state itself is parameterized as a deep, learned architecture.\nBy storing the gradients for the surprisal-based loss, our neural memory module is essentially trained to remember all the prior attention outputs.\n\n\n\nPurple: frozen LLaMA. Green: trainable NMM + fast gradients. Only the memory “cartridge” adapts. The backbone is completely frozen: only ~2% of parameters are trainable.\n\n\nFinally, we propagate the language modeling loss over the weights of the NMMs and actually perform weight updates. It’s important to note here that the rest of the weights in the pre-trained backbone are frozen and no updates are made to them. We can think of these “slow updates” as teaching the NMMs how to recall best.\n\nAttention Distillation Loss\nIn practice, we noticed that the pure language modeling objective was really easy for NMMs to master. And in fact, we do much better than our out-of-the-box LLM when evaluating next token prediction purely on a token accuracy / perplexity basis. However, when it came to the challenging language benchmarks that pre-trained LLMs excel at, solely using the language modeling loss was insufficient.\nIn our setting, we want the NMM to first recover baseline capabilities of the backbone LLM. We supervise the outputs of the Titan-agumented layers with that from our pre-trained backbone. In effect, we implicitly distill the expressivity from our pre-trained “teacher attention” into our adapter’s recollection strategy only using a single additional forward pass at train-time.\n\n\n\n\n\n\nCalculating Attention Distillation Loss\n\n\n\n\n\n\nCompute normal titan-llama forward pass\n\nCollect hidden states, these are “student” latents\n\nDo a forward pass over the backbone, without segmented attn\n\nCollect hidden states, these are “teacher” latents\n\nCompute MSE over these latents, scale, and add to LM loss"
  },
  {
    "objectID": "posts/titan-llama/index.html#experiments",
    "href": "posts/titan-llama/index.html#experiments",
    "title": "Titan-LLaMA: Neural Memory Adapters for Continual Learning",
    "section": "Experiments",
    "text": "Experiments\n\n\n\n\n\n\nImportant\n\n\n\nHypothesis: A Titans-style neural memory, used as an adapter on top of a frozen LLM backbone, can\n\nrecover backbone LLM capabilities\n\nbe fine-tuned to specialize in new domains\n\n\n\nExperiment 1: Can Titan-LLaMA recover base LM abilities?\nWe test whether the NMM can compensate for the information lost when replacing full attention with cheap segmented attention.\n1a. Language Modeling: Train the Titan-LLaMA adapter on 1B tokens from SlimPajama-627B and FineWeb-EDU. Report validation perplexity and token accuracy.\n1b. NLP Benchmarks: Evaluate on Winogrande [@sakaguchi2019winogrande], BoolQ [@clark2019boolq], CommonsenseQA [@talmor2018commonsenseqa], DROP [@dua2019drop], SQuAD [@rajpurkar2016squad], and PubMedQA [@jin2019pubmedqa]. Compare against frozen LLaMA (full attention) and segmented-only (ablation).\nExperiment 2: Domain Adaptation\nCan we train domain-specific NMMs that specialize to new areas?\n\nBiomed: PubMed corpus, PubMedQA [@jin2019pubmedqa]\nLegal: LexGLUE benchmark [@chalkidis2021lexglue]\nMath: AQUA-RAT word problems [@ling2017aqua]\n\n\n\n\n\n\n\nNote\n\n\n\nWe test both recovery (can NMM fix segmentation?) and adaptation (can NMM specialize?)."
  },
  {
    "objectID": "posts/titan-llama/index.html#results",
    "href": "posts/titan-llama/index.html#results",
    "title": "Titan-LLaMA: Neural Memory Adapters for Continual Learning",
    "section": "Results",
    "text": "Results\n\nExperimental Setup\nWe use Llama-3.1-8b as our pre-trained LLM backbone. For the NMMs, we attach 2-layer MLPs, with hidden dimension 2048 onto layers 4, 8, 12, 16, and 20. We use an attention segment length half the size of the data sequence length, a neural memory segment length of 64, and a neural memory batch size of 64.\nFor pre-training data, we use a 1b-token mixture of Slim Pajama and FineWeb-EDU. We fine-tune on the training sets of PubMedQA, CaseHOLD, AQUA-RAT.\nWe find that higher neural learning rates (on the order of 1e-3) are helpful to performance. Gradient updates are performed with a learning rate of 3e-4 with a linear warmup and cosine annealing. Further, we weigh the distillation loss at 0.6 relative to language modeling loss. Due to the scope of the project, we were unable to thoroughly vet our hyperparameter setup–further work will explore annealing distillation loss weight and more tuning.\n\n\nExperiment 1 Results\nWe verified that imparing attention masks led to catastrophic results. Performance of Llama-Titan exceeded that of the frozen backbone on next-token prediction / language modeling.\n\n\n\nModel\nToken Accuracy\nPerplexity\n\n\n\n\nFull-Attention LLaMA\n55.2%\n7.26\n\n\nSegmented-only (no NMM)\n0%\n1,516,042\n\n\nTitan-LLaMA (Segmented + NMM)\n100%\n~1.0\n\n\n\nThe NMM not only recovers but actually exceeds baseline language modeling metrics—achieving 100% token accuracy vs. 55.2% for the original model. This suggests the NMM learns an effective compression of the training distribution.\nThis result validates the first part of our hypothesis—the NMM can recover capabilities lost to segmentation, at least for the language modeling objective. However, perfect scores (100% accuracy, ~1.0 perplexity) suggest possible overfitting to the training distribution rather than true generalization. This motivates our next experiment: do these gains transfer to downstream benchmarks?\nBuilding on the language modeling success, we add attention distillation (described in Methods) to transfer rich representations to downstream tasks, and we achieve the following results:\n\n\n\n\n\n\n\n\n\nBenchmark\nFull-Attn LLaMA\nSegmented-only\nTitan-LLaMA (LM loss)\n\n\n\n\nWinogrande\n60.5%\n48.4%\n52.3% (+3.9%)\n\n\nBoolQ\n75.0%\n43%\n63% (+20%)\n\n\nCommonsenseQA\n75.0%\n13%\n18% (+5%)\n\n\nDROP\n59.5%\n0%\n7.4% (+7.4%)\n\n\nSquadV2\n77%\n0%\n6.25% (+6.25%)\n\n\n\nWe therefore conclude that LM loss alone teaches the NMM to predict tokens, but not to build the rich intermediate representations that full attention provides. Attention distillation forces the NMM’s hidden states to match the teacher model’s representations, which is necessary for successful downstream task performance.\n\n\n\n\n\n\nNote\n\n\n\nNote that only Winogrande and BoolQ were properly hyperparameter-tuned; for the other tasks we only ran ~30 training steps, so their scores are likely under-optimized.\n\n\nThese results partially validate our hypothesis. The NMM does recover some benchmark performance (e.g., BoolQ improves by 20 percentage points over segmented-only), but does not fully match the frozen backbone. This gap could come from several factors… limited hyperparameter tuning, insufficient training compute, suboptimal NMM architecture choices (e.g., 2-layer MLPs may be too shallow), or potentially fundamental capacity limitations of the approach. Disentangling these factors would require more extensive ablations. The consistent improvement from distillation does confirm that representation quality, not just next-token prediction, is critical for downstream tasks.\n\n\nExperiment 2 Results\nDomain-specific fine-tuning. We fine-tuned Titan-LLaMA on 3 domain-specific tasks:\n\n\n\n\n\n\n\n\n\nBenchmark\nFull-Attn LLaMA\nSegmented-only\nTitan-LLaMA (fine-tuned)\n\n\n\n\nPubMedQA (Biomed)\n58%\n46%\n55%\n\n\nCaseHOLD (Legal)\n33%\n19%\n23%\n\n\nAQUA-RAT (Math)\n28%\n22%\n24.8%\n\n\n\nThese results validate the second part of our hypothesis—the NMM can be fine-tuned to specialize in new domains. The varying recovery rates (75% for PubMedQA vs. 29% for CaseHOLD) suggest that some domains are easier to compress into learned memory than others.\nPubMedQA shows the strongest recovery, closing 75% of the gap between segmented-only and full attention (from 46% to 55%, vs. 58% full attention). CaseHOLD and AQUA-RAT show more modest but still meaningful improvements."
  },
  {
    "objectID": "posts/titan-llama/index.html#limitations-and-future-work",
    "href": "posts/titan-llama/index.html#limitations-and-future-work",
    "title": "Titan-LLaMA: Neural Memory Adapters for Continual Learning",
    "section": "Limitations and Future Work",
    "text": "Limitations and Future Work\n1) Incomplete recovery. Even with distillation, we don’t fully recover full-attention performance on most benchmarks. The gap suggests that some information encoded in full attention is fundamentally difficult to compress into a learned memory module. Future work could explore larger NMM architectures or different distillation strategies.\n2) Throughput overhead. Our current NMM implementation introduces computational overhead that negates the theoretical efficiency gains of segmented attention. As shown in Figure 1, the segmented+NMM model currently underperforms full attention in throughput. Optimized GPU and better memory management could address this.\n3) Limited hyperparameter tuning. Due to compute and time constraints, only Winogrande and BoolQ were thoroughly tuned. Other benchmarks (CommonsenseQA, DROP, SQuAD) ran for ~30 training steps, so their scores are likely under-optimized. More extensive sweeps could improve results.\n4) Scale. All experiments use LLaMA-3.1-8B. We would like to test whether our findings generalize to larger models where the capacity gap between NMM and full attention may be more or less pronounced.\n5) Future directions. Promising next steps include: (1) training NMMs from distillation alone without LM loss, (2) exploring different NMM architectures beyond 2-layer MLPs, (3) investigating whether NMMs can enable genuine continual learning without catastrophic forgetting, and (4) scaling to longer sequences where the efficiency benefits of segmentation become more pronounced."
  },
  {
    "objectID": "posts/titan-llama/index.html#references",
    "href": "posts/titan-llama/index.html#references",
    "title": "Titan-LLaMA: Neural Memory Adapters for Continual Learning",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "posts/titan-llama/index.html#bibliography",
    "href": "posts/titan-llama/index.html#bibliography",
    "title": "Titan-LLaMA: Neural Memory Adapters for Continual Learning",
    "section": "Bibliography",
    "text": "Bibliography\n@hu2021lora Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., & Chen, W. (2021). LoRA: Low-Rank Adaptation of Large Language Models. arXiv preprint arXiv:2106.09685.\n@hewitt2024cartridges Hewitt, J., et al. (2024). Cartridges: Compact Representations for LLM Reasoning. arXiv preprint arXiv:2506.06266.\n@gu2021efficiently Gu, A., Goel, K., & Ré, C. (2021). Efficiently Modeling Long Sequences with Structured State Spaces. arXiv preprint arXiv:2111.00396.\n@behrouz2024titans Behrouz, A., et al. (2024). Titans: Learning to Memorize at Test Time. Google Research. arXiv preprint arXiv:2501.00663.\n@sakaguchi2019winogrande Sakaguchi, K., et al. (2019). WinoGrande: An Adversarial Winograd Schema Challenge at Scale. arXiv preprint arXiv:1907.10641.\n@clark2019boolq Clark, C., et al. (2019). BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions. arXiv preprint arXiv:1905.10044.\n@talmor2018commonsenseqa Talmor, A., et al. (2018). CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge. arXiv preprint arXiv:1811.00937.\n@dua2019drop Dua, D., et al. (2019). DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning. arXiv preprint arXiv:1903.00161.\n@rajpurkar2016squad Rajpurkar, P., et al. (2016). SQuAD: 100,000+ Questions for Machine Comprehension of Text. arXiv preprint arXiv:1606.05250.\n@jin2019pubmedqa Jin, Q., et al. (2019). PubMedQA: A Dataset for Biomedical Research Question Answering. arXiv preprint arXiv:1909.06146.\n@chalkidis2021lexglue Chalkidis, I., et al. (2021). LexGLUE: A Benchmark Dataset for Legal Language Understanding. arXiv preprint arXiv:2110.00976.\n@ling2017aqua Ling, W., et al. (2017). AQUA-RAT: Algebra Question Answering Dataset. GitHub repository.\n@behrouz2025podcast Behrouz, A. (2025). Cognitive Revolution podcast. YouTube."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rabbitholes",
    "section": "",
    "text": "Welcome to my technical blog! Here I share insights on AI, machine learning, research projects, and other topics that interest me."
  },
  {
    "objectID": "index.html#recent-posts",
    "href": "index.html#recent-posts",
    "title": "Rabbitholes",
    "section": "Recent Posts",
    "text": "Recent Posts"
  }
]