<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.555">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Sarah Pan">
<meta name="author" content="Sarah Dufays">
<meta name="dcterms.date" content="2025-12-13">

<title>Rabbitholes - Titan-LLaMA: Neural Memory Adapters for Continual Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Rabbitholes</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-page-left">
      <h1 class="title">Titan-LLaMA: Neural Memory Adapters for Continual Learning</h1>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta column-page-left">

      <div>
      <div class="quarto-title-meta-heading">Authors</div>
      <div class="quarto-title-meta-contents">
               <p>Sarah Pan </p>
               <p>Sarah Dufays </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">December 13, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#motivation" id="toc-motivation" class="nav-link active" data-scroll-target="#motivation">Motivation</a></li>
  <li><a href="#background" id="toc-background" class="nav-link" data-scroll-target="#background">Background</a>
  <ul class="collapse">
  <li><a href="#titans-deep-neural-memory-as-state" id="toc-titans-deep-neural-memory-as-state" class="nav-link" data-scroll-target="#titans-deep-neural-memory-as-state">Titans: Deep Neural Memory as State</a></li>
  </ul></li>
  <li><a href="#methods" id="toc-methods" class="nav-link" data-scroll-target="#methods">Methods</a>
  <ul class="collapse">
  <li><a href="#attention-distillation-loss" id="toc-attention-distillation-loss" class="nav-link" data-scroll-target="#attention-distillation-loss">Attention Distillation Loss</a></li>
  </ul></li>
  <li><a href="#experiments" id="toc-experiments" class="nav-link" data-scroll-target="#experiments">Experiments</a></li>
  <li><a href="#results" id="toc-results" class="nav-link" data-scroll-target="#results">Results</a>
  <ul class="collapse">
  <li><a href="#experimental-setup" id="toc-experimental-setup" class="nav-link" data-scroll-target="#experimental-setup">Experimental Setup</a></li>
  <li><a href="#experiment-1-results" id="toc-experiment-1-results" class="nav-link" data-scroll-target="#experiment-1-results">Experiment 1 Results</a></li>
  <li><a href="#experiment-2-results" id="toc-experiment-2-results" class="nav-link" data-scroll-target="#experiment-2-results">Experiment 2 Results</a></li>
  </ul></li>
  <li><a href="#limitations-and-future-work" id="toc-limitations-and-future-work" class="nav-link" data-scroll-target="#limitations-and-future-work">Limitations and Future Work</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block column-page-left" id="quarto-document-content">





<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>This work was submitted as our final project for Deep Learning at MIT (6.7960). Much of the results are still preliminary and poorly ablated as work is ongoing.</p>
</div>
</div>
<section id="motivation" class="level2">
<h2 class="anchored" data-anchor-id="motivation">Motivation</h2>
<p>We read the Titans paper and really liked it, but we didn’t want to train an end-to-end architecture from scratch. So instead, we came up with a new architecture that gives us the best of both worlds.</p>
<p>Off-the-shelf LLMs are extremely strong, but their expressivity is expensive as “memory” grows quadratically with sequence length. Among many others, Flash attention, sliding attention windows, linear attention, and KV-caching are techniques that provide speedups, but we were curious to know if we could <strong>attach a learned state during post training for stateful, sub-quadratic memory</strong>.</p>
<p>Titans is appealing in this regard because its memory is a deep architecture that accumulates information across full-attention segments. Rather than training a Titan model from scratch, we treat neural memory as a modular adapter that can be attached to a pretrained backbone whose long-range attention has been intentionally restricted.</p>
<p>Concretely, we freeze a pretrained LLaMA model, replace full attention with segmented attention, and attach a Titans-style Neural Memory Module (NMM) that serves as the sole mechanism for cross-segment information flow. The memory module functions as a drop-in “cartridge” that can be trained or swapped independently while the backbone remains fixed. To our knowledge, this is the first exploration of Titans-style neural memory used in this post-training, frozen-backbone regime.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> The result is an architecture that (i) generates tokens much faster as generation length grows and (ii) can be fine-tuned by swapping in task-specific memory adapters for downstream domains.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/throughput_vs_generation.png" class="img-fluid figure-img"></p>
<figcaption>Token throughput vs.&nbsp;generation length. Full-attention LLaMA slows down as sequences grow, while segmented attention keeps throughput roughly flat; we’ll keep you posted if either of us learns enough CUDA to engineer kernels for this.</figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Our prototype builds on an existing open-source Titans implementation that prioritizes correctness.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> The neural memory kernels are probably under-optimized, introducing overhead that causes throughput to lag slightly behind full attention in practice. We consider this an engineering issue and expect substantial speedups from better implementation.</p>
</div>
</div>
</section>
<section id="background" class="level2">
<h2 class="anchored" data-anchor-id="background">Background</h2>
<p>Our inspiration for this work falls into two categories.</p>
<p><strong>Memory and state.</strong> RNNs and LSTMs compress history into a fixed-size hidden vector but eventually forget distant information and cannot be parallelized. SSMs <span class="citation" data-cites="gu2021efficiently">(<a href="#ref-gu2021efficiently" role="doc-biblioref">Gu, Goel, and Ré 2021</a>)</span> revisit this idea by improving stability and parallelizability. Transformers on the other hand store the entire sequence explicitly, relying on an attention “state” that grows quadratically with sequence length. For the record, we think it’s totally understated how extremely parallelizable and expressive transformers are–but mitigating memory consumption is still a key concern.</p>
<p><strong>Parameter-efficient adapters.</strong> Full fine-tuning is expensive and risks catastrophic forgetting. LoRA <span class="citation" data-cites="hu2021lora">(<a href="#ref-hu2021lora" role="doc-biblioref">Hu et al. 2021</a>)</span> instead adds trainable low-rank factors to frozen weight matrices, which reduces the trainable parameters by orders of magnitude while preserving expressivity. Prefix-tuning <span class="citation" data-cites="li2021prefix">(<a href="#ref-li2021prefix" role="doc-biblioref">Li and Liang 2021</a>)</span> prepends learned continuous prompts to the input sequence, allowing the model to behave as if conditioned on specific tasks or domains while keeping the backbone frozen.</p>
<section id="titans-deep-neural-memory-as-state" class="level3">
<h3 class="anchored" data-anchor-id="titans-deep-neural-memory-as-state">Titans: Deep Neural Memory as State</h3>
<p><strong>Titans: deep neural memory as state.</strong> Titans <span class="citation" data-cites="behrouz2024titans">(<a href="#ref-behrouz2024titans" role="doc-biblioref">Behrouz et al. 2024</a>)</span> combines recurrent architectures as long-term memory with localized attention windows as short-term memory. These were the three things that we thought were the most important to understand from the paper.</p>
<p><strong>1. Titans as a Mega-Abstraction of LSTMs.</strong> Each cell state is a sub-sequence of tokens, but instead of forget gates and input gates, an MLP learns how to incorporate/get rid of information across a sequence. In other words, instead of carrying some <span class="math inline">\(h_t\)</span> forward, Titans carry a long-term memory module <span class="math inline">\(M_t\)</span> whose weights + fast gradient updates determine the state.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/titans_overview.png" class="img-fluid figure-img"></p>
<figcaption>Reading from memory looks like “querying the state” via a forward pass, and updating the state looks like propgating a loss backward.</figcaption>
</figure>
</div>
<p><strong>2. Fast and Slow Updates.</strong> Long-term memory aims to memorize what happened at a previous attention window. Titans defines a “fast” memory loss <span class="math inline">\(l(\mathcal{M}_{t -1}; x_t) = || q_t - k_t ||^2\)</span> and updates its state over chunks in a sequence as such</p>
<p><span id="eq-surprise-update"><span class="math display">\[
\mathcal{M}_t = (1 - \alpha_t) \mathcal{M}_{t - 1} + S_t \\
\tag{1}\]</span></span> <span class="math display">\[
S_t = \eta_t S_{t - 1} - \theta_t \nabla l(\mathcal{M}_{t -1}; x_t).
\]</span></p>
<p>The surprise update rule is shown in <a href="#eq-surprise-update" class="quarto-xref">Equation&nbsp;1</a>. Here, <span class="math inline">\(\alpha_t\)</span> serves as the gating mechanism, like that in RNNs.</p>
<p>On the flip side, we still care about the language modeling objective and want to update the weights that parameterize <span class="math inline">\(\mathcal{M}_t\)</span> to optimize retrieval of the right information. These are the “slow updates” that operate on the timescale of our optimizer.</p>
<p><strong>3. This Can be Made Practical.</strong> A naive implementation of these per-token weight updates is inherently sequential, but with a few tricks, you can actually vectorize much of the computation. If chunks within a sequence have length <code>b</code> and hidden size <code>d</code>, the rest of the computation is described as such:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># X: [N, b, d]   (N chunks of b tokens)</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>u_prev <span class="op">=</span> <span class="dv">0</span>                      <span class="co"># last "update" state for this param (shape = param_shape)</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>m_prev[o] <span class="op">=</span> <span class="dv">0</span> <span class="cf">for</span> o <span class="kw">in</span> <span class="fl">1.</span>.O     <span class="co"># last momentum states</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(N):</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 1) FAST write signal inside the chunk (parallel over b tokens)</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    <span class="co">#    think: per-token grad of an associative loss wrt memory params</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    s_tok <span class="op">=</span> grad_mem_loss(M, X[t])            <span class="co"># shape: [b, param_shape]   (conceptually)</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># (optional) reduce tokens -&gt; one chunk surprise (still parallel)</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    s <span class="op">=</span> sum_i theta[t,i] <span class="op">*</span> s_tok[i]           <span class="co"># shape: [param_shape]</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 2) SLOW accumulator across chunks (scan in t)</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> o <span class="kw">in</span> <span class="fl">1.</span>.O:</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>        m[o] <span class="op">=</span> eta[o,t] <span class="op">*</span> m_prev[o] <span class="op">+</span> (s <span class="cf">if</span> o<span class="op">==</span><span class="dv">1</span> <span class="cf">else</span> m[o<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    u <span class="op">=</span> sum_o comb[o,t] <span class="op">*</span> m[o]                <span class="co"># (this is the “einsum combine orders” idea)</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 3) FORGET/DECAY across chunks (another scan)</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    u <span class="op">=</span> (<span class="dv">1</span> <span class="op">-</span> alpha[t]) <span class="op">*</span> u_prev <span class="op">+</span> u</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    apply_update_to_memory(M, u)              <span class="co"># e.g., M &lt;- M + u   (sign convention depends)</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    u_prev, m_prev <span class="op">=</span> u, m</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="methods" class="level2">
<h2 class="anchored" data-anchor-id="methods">Methods</h2>
<p><strong>Our perspective:</strong> We combine these views by attaching Titan-style memory to a frozen LLaMA backbone and treating it as a cartridge-style adapter. The backbone stays fixed; only the Titan memory continues learning.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/nmm_bridge.png" class="img-fluid figure-img"></p>
<figcaption>The NMM (green) injects cross-segment attention entries, allowing information retrieval across segment boundaries without full-sequence attention.</figcaption>
</figure>
</div>
<p>The next step of the process is to augment the localized attention calculation with “long-term memory” that is retrieved from the NMM. The NMM will try to recall what happened at the previous local attention window.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/temporal_loop.png" class="img-fluid figure-img"></p>
<figcaption>As chunks c1–c5 progress, the NMM is queried then updated based on prediction error, making information persistent across the full sequence.</figcaption>
</figure>
</div>
<p>It might be helpful to “flip” the time axis when thinking about the fast update process. If we imagine each localized attention segment to be its own element in a sequence, this process closely resembles that of an LSTM or RNN. However, instead of learning weights that modulate previous hidden cell states, the hidden state itself is parameterized as a deep, learned architecture.</p>
<p>By storing the gradients for the surprisal-based loss, our neural memory module is essentially trained to remember all the prior attention outputs.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/full_architecture.png" class="img-fluid figure-img"></p>
<figcaption>Purple: frozen LLaMA. Green: trainable NMM + fast gradients. Only the memory “cartridge” adapts. The backbone is completely frozen: only ~2% of parameters are trainable.</figcaption>
</figure>
</div>
<p>Finally, we propagate the language modeling loss over the weights of the NMMs and actually perform weight updates. It’s important to note here that the rest of the weights in the pre-trained backbone are frozen and no updates are made to them. We can think of these “slow updates” as teaching the NMMs <em>how</em> to recall best.</p>
<section id="attention-distillation-loss" class="level3">
<h3 class="anchored" data-anchor-id="attention-distillation-loss">Attention Distillation Loss</h3>
<p>In practice, we noticed that the pure language modeling objective was really easy for NMMs to master. And in fact, we do much better than our out-of-the-box LLM when evaluating next token prediction purely on a token accuracy / perplexity basis. However, when it came to the challenging language benchmarks that pre-trained LLMs excel at, solely using the language modeling loss was insufficient.</p>
<p>In our setting, we want the NMM to first recover baseline capabilities of the backbone LLM. We supervise the outputs of the Titan-agumented layers with that from our pre-trained backbone. In effect, we implicitly distill the expressivity from our pre-trained “teacher attention” into our adapter’s recollection strategy only using a single additional forward pass at train-time.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Calculating Attention Distillation Loss
</div>
</div>
<div class="callout-body-container callout-body">
<ol type="1">
<li>Compute normal forward pass
<ol type="a">
<li>Collect hidden states, these are “student” latents</li>
</ol></li>
<li>Do a forward pass over the backbone, without segmented attn
<ol type="a">
<li>Collect hidden states, these are “teacher” latents</li>
</ol></li>
<li>Compute MSE over these latents, scale, and add to LM loss</li>
</ol>
</div>
</div>
</section>
</section>
<section id="experiments" class="level2">
<h2 class="anchored" data-anchor-id="experiments">Experiments</h2>
<p><strong>Hypothesis:</strong> A Titans-style neural memory, used as an adapter on top of a frozen LLM backbone, can</p>
<p><strong>(1)</strong> recover backbone LLM capabilities<br>
<strong>(2)</strong> and be fine-tuned to specialize in new domains.</p>
<p><strong>Experiment 1: Can Titan-LLaMA recover base LM abilities?</strong></p>
<p>We test whether the NMM can compensate for the information lost when replacing full attention with cheap segmented attention.</p>
<p><em>1a. Language Modeling:</em> Train the Titan-LLaMA adapter on 1B tokens from SlimPajama-627B and FineWeb-EDU. Report validation perplexity and token accuracy.</p>
<p><em>1b. NLP Benchmarks:</em> Evaluate on Winogrande <span class="citation" data-cites="sakaguchi2019winogrande">(<a href="#ref-sakaguchi2019winogrande" role="doc-biblioref">Sakaguchi et al. 2020</a>)</span>, BoolQ <span class="citation" data-cites="clark2019boolq">(<a href="#ref-clark2019boolq" role="doc-biblioref">Clark et al. 2019</a>)</span>, CommonsenseQA <span class="citation" data-cites="talmor2018commonsenseqa">(<a href="#ref-talmor2018commonsenseqa" role="doc-biblioref">Talmor et al. 2018</a>)</span>, DROP <span class="citation" data-cites="dua2019drop">(<a href="#ref-dua2019drop" role="doc-biblioref">Dua et al. 2019</a>)</span>, SQuAD <span class="citation" data-cites="rajpurkar2016squad">(<a href="#ref-rajpurkar2016squad" role="doc-biblioref">Rajpurkar et al. 2016</a>)</span>, and PubMedQA <span class="citation" data-cites="jin2019pubmedqa">(<a href="#ref-jin2019pubmedqa" role="doc-biblioref">Jin et al. 2019</a>)</span>. Compare against frozen LLaMA (full attention) and segmented-only (ablation).</p>
<p><strong>Experiment 2: Domain Adaptation</strong></p>
<p>Can we train domain-specific NMMs that specialize to new areas?</p>
<ul>
<li><strong>Biomed:</strong> PubMed corpus, PubMedQA <span class="citation" data-cites="jin2019pubmedqa">(<a href="#ref-jin2019pubmedqa" role="doc-biblioref">Jin et al. 2019</a>)</span></li>
<li><strong>Legal:</strong> LexGLUE benchmark <span class="citation" data-cites="chalkidis2021lexglue">(<a href="#ref-chalkidis2021lexglue" role="doc-biblioref">Chalkidis et al. 2021</a>)</span></li>
<li><strong>Math:</strong> AQUA-RAT word problems <span class="citation" data-cites="ling2017aqua">(<a href="#ref-ling2017aqua" role="doc-biblioref">Ling et al. 2017</a>)</span></li>
</ul>
</section>
<section id="results" class="level2">
<h2 class="anchored" data-anchor-id="results">Results</h2>
<section id="experimental-setup" class="level3">
<h3 class="anchored" data-anchor-id="experimental-setup">Experimental Setup</h3>
<p>We use <a href="https://huggingface.co/meta-llama/Llama-3.1-8B">Llama-3.1-8b</a> as our pre-trained LLM backbone. For the NMMs, we attach 2-layer MLPs, with hidden dimension 2048 onto layers 4, 8, 12, 16, and 20. We use an attention segment length half the size of the data sequence length, a neural memory segment length of 64, and a neural memory batch size of 64.</p>
<p>For pre-training data, we use a 1b-token mixture of <a href="https://huggingface.co/datasets/cerebras/SlimPajama-627B">Slim Pajama</a> and <a href="https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu">FineWeb-EDU</a>. We fine-tune on the training sets of <a href="https://huggingface.co/datasets/qiaojin/PubMedQA">PubMedQA</a>, <a href="https://huggingface.co/datasets/casehold/casehold">CaseHOLD</a>, <a href="https://huggingface.co/datasets/deepmind/aqua_rat">AQUA-RAT</a>.</p>
<p>We find that higher neural learning rates (on the order of 1e-3) are helpful to performance. Gradient updates are performed with a learning rate of 3e-4 with a linear warmup and cosine annealing. Further, we weigh the distillation loss at 0.6 relative to language modeling loss. Due to the scope of the project, we were unable to thoroughly vet our hyperparameter setup–further work will explore annealing distillation loss weight and more tuning.</p>
</section>
<section id="experiment-1-results" class="level3">
<h3 class="anchored" data-anchor-id="experiment-1-results">Experiment 1 Results</h3>
<p>We verified that imparing attention masks led to catastrophic results. Performance of Llama-Titan exceeded that of the frozen backbone on next-token prediction / language modeling.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Model</th>
<th>Token Accuracy</th>
<th>Perplexity</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Full-Attention LLaMA</td>
<td>55.2%</td>
<td>7.26</td>
</tr>
<tr class="even">
<td>Segmented-only (no NMM)</td>
<td>0%</td>
<td>1,516,042</td>
</tr>
<tr class="odd">
<td><strong>Titan-LLaMA (Segmented + NMM)</strong></td>
<td><strong>100%</strong></td>
<td><strong>~1.0</strong></td>
</tr>
</tbody>
</table>
<p>The NMM not only recovers but actually exceeds baseline language modeling metrics—achieving 100% token accuracy vs.&nbsp;55.2% for the original model. This suggests the NMM learns an effective compression of the training distribution.</p>
<p>This result validates the first part of our hypothesis—the NMM can recover capabilities lost to segmentation, at least for the language modeling objective. However, perfect scores (100% accuracy, ~1.0 perplexity) suggest possible overfitting to the training distribution rather than true generalization. This motivates our next experiment: do these gains transfer to downstream benchmarks?</p>
<p>Building on the language modeling success, we add attention distillation (described in Methods) to transfer rich representations to downstream tasks, and we achieve the following results:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 16%">
<col style="width: 25%">
<col style="width: 23%">
<col style="width: 35%">
</colgroup>
<thead>
<tr class="header">
<th>Benchmark</th>
<th>Full-Attn LLaMA</th>
<th>Segmented-only</th>
<th>Titan-LLaMA (LM loss)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Winogrande</strong></td>
<td>60.5%</td>
<td>48.4%</td>
<td>52.3% (+3.9%)</td>
</tr>
<tr class="even">
<td><strong>BoolQ</strong></td>
<td>75.0%</td>
<td>43%</td>
<td>63% (+20%)</td>
</tr>
<tr class="odd">
<td>CommonsenseQA</td>
<td>75.0%</td>
<td>13%</td>
<td>18% (+5%)</td>
</tr>
<tr class="even">
<td>DROP</td>
<td>59.5%</td>
<td>0%</td>
<td>7.4% (+7.4%)</td>
</tr>
<tr class="odd">
<td>SquadV2</td>
<td>77%</td>
<td>0%</td>
<td>6.25% (+6.25%)</td>
</tr>
</tbody>
</table>
<p>We therefore conclude that LM loss alone teaches the NMM to predict tokens, but not to build the rich intermediate representations that full attention provides. Attention distillation forces the NMM’s hidden states to match the teacher model’s representations, which is necessary for successful downstream task performance.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Note that <strong>only Winogrande and BoolQ were properly hyperparameter-tuned</strong>; for the other tasks we only ran ~30 training steps<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>, so their scores are very under-optimized.</p>
</div>
</div>
<p>These results partially validate our hypothesis. The NMM does recover some benchmark performance (e.g., BoolQ improves by 20 percentage points over segmented-only), but does not fully match the frozen backbone. This gap could come from several factors… limited hyperparameter tuning, insufficient training compute, suboptimal NMM architecture choices (e.g., 2-layer MLPs may be too shallow), or potentially fundamental capacity limitations of the approach. Disentangling these factors would require more extensive ablations. The consistent improvement from distillation does confirm that representation quality, not just next-token prediction, is critical for downstream tasks.</p>
</section>
<section id="experiment-2-results" class="level3">
<h3 class="anchored" data-anchor-id="experiment-2-results">Experiment 2 Results</h3>
<p><strong>Domain-specific fine-tuning.</strong> We fine-tuned Titan-LLaMA on 3 domain-specific tasks:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 15%">
<col style="width: 23%">
<col style="width: 22%">
<col style="width: 38%">
</colgroup>
<thead>
<tr class="header">
<th>Benchmark</th>
<th>Full-Attn LLaMA</th>
<th>Segmented-only</th>
<th>Titan-LLaMA (fine-tuned)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>PubMedQA (Biomed)</td>
<td>58%</td>
<td>46%</td>
<td>55%</td>
</tr>
<tr class="even">
<td>CaseHOLD (Legal)</td>
<td>33%</td>
<td>19%</td>
<td>23%</td>
</tr>
<tr class="odd">
<td>AQUA-RAT (Math)</td>
<td>28%</td>
<td>22%</td>
<td>24.8%</td>
</tr>
</tbody>
</table>
<p>These results validate the second part of our hypothesis—the NMM can be fine-tuned to specialize in new domains. The varying recovery rates (75% for PubMedQA vs.&nbsp;29% for CaseHOLD) suggest that some domains are easier to compress into learned memory than others.</p>
<p>PubMedQA shows the strongest recovery, closing 75% of the gap between segmented-only and full attention (from 46% to 55%, vs.&nbsp;58% full attention). CaseHOLD and AQUA-RAT show more modest but still meaningful improvements.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
</section>
</section>
<section id="limitations-and-future-work" class="level2">
<h2 class="anchored" data-anchor-id="limitations-and-future-work">Limitations and Future Work</h2>
<p><strong>1) Incomplete recovery.</strong> Even with distillation, we don’t fully recover full-attention performance on most benchmarks. The gap suggests that some information encoded in full attention is fundamentally difficult to compress into a learned memory module. Future work could explore larger NMM architectures or different distillation strategies.</p>
<p><strong>2) Throughput overhead.</strong> Our current NMM implementation introduces computational overhead that negates the theoretical efficiency gains of segmented attention. As shown in Figure 1, the segmented+NMM model currently underperforms full attention in throughput. Optimized GPU and better memory management could address this.</p>
<p><strong>3) Limited hyperparameter tuning.</strong> Due to compute and time constraints, only Winogrande and BoolQ were thoroughly tuned. Other benchmarks (CommonsenseQA, DROP, SQuAD) ran for ~30 training steps, so their scores are likely under-optimized. More extensive sweeps could improve results. We are particularly excited about distillation loss annealing.</p>
<p><strong>4) Scale.</strong> All experiments use LLaMA-3.1-8B. We would like to test whether our findings generalize to larger models where the capacity gap between NMM and full attention may be more or less pronounced.</p>
<p><strong>5) Future directions.</strong> Promising next steps include: (1) training NMMs from distillation alone without LM loss, (2) exploring different NMM architectures beyond 2-layer MLPs, (3) investigating whether NMMs can enable genuine continual learning without catastrophic forgetting, and (4) scaling to longer sequences where the efficiency benefits of segmentation become more pronounced.</p>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-behrouz2024titans" class="csl-entry" role="listitem">
Behrouz, Ali, Mahdyar Bondaschi, Mehrdad Farhadi, Michael Horton, and Arno Blaas Cremers. 2024. <span>“Titans: Learning to Memorize at Test Time.”</span> <em>arXiv Preprint arXiv:2501.00663</em>.
</div>
<div id="ref-chalkidis2021lexglue" class="csl-entry" role="listitem">
Chalkidis, Ilias, Abhik Jana, Dirk Hartung, Michael Bommarito, Ion Androutsopoulos, Daniel Katz, and Nikolaos Aletras. 2021. <span>“LexGLUE: A Benchmark Dataset for Legal Language Understanding in English.”</span> <em>arXiv Preprint arXiv:2110.00976</em>.
</div>
<div id="ref-clark2019boolq" class="csl-entry" role="listitem">
Clark, Christopher, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. 2019. <span>“BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions.”</span> <em>arXiv Preprint arXiv:1905.10044</em>.
</div>
<div id="ref-dua2019drop" class="csl-entry" role="listitem">
Dua, Dheeru, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2019. <span>“DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning over Paragraphs.”</span> <em>arXiv Preprint arXiv:1903.00161</em>.
</div>
<div id="ref-gu2021efficiently" class="csl-entry" role="listitem">
Gu, Albert, Karan Goel, and Christopher Ré. 2021. <span>“Efficiently Modeling Long Sequences with Structured State Spaces.”</span> <em>arXiv Preprint arXiv:2111.00396</em>.
</div>
<div id="ref-hu2021lora" class="csl-entry" role="listitem">
Hu, Edward J, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. <span>“LoRA: Low-Rank Adaptation of Large Language Models.”</span> <em>arXiv Preprint arXiv:2106.09685</em>.
</div>
<div id="ref-jin2019pubmedqa" class="csl-entry" role="listitem">
Jin, Qiao, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. 2019. <span>“PubMedQA: A Dataset for Biomedical Research Question Answering.”</span> <em>arXiv Preprint arXiv:1909.06146</em>.
</div>
<div id="ref-li2021prefix" class="csl-entry" role="listitem">
Li, Xiang Lisa, and Percy Liang. 2021. <span>“Prefix-Tuning: Optimizing Continuous Prompts for Generation.”</span> <em>arXiv Preprint arXiv:2101.00190</em>.
</div>
<div id="ref-ling2017aqua" class="csl-entry" role="listitem">
Ling, Wang, Dani Yogatama, Chris Dyer, and Phil Blunsom. 2017. <span>“Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems.”</span> <em>arXiv Preprint arXiv:1705.04146</em>.
</div>
<div id="ref-rajpurkar2016squad" class="csl-entry" role="listitem">
Rajpurkar, Pranav, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. <span>“Squad: 100,000+ Questions for Machine Comprehension of Text.”</span> <em>arXiv Preprint arXiv:1606.05250</em>.
</div>
<div id="ref-sakaguchi2019winogrande" class="csl-entry" role="listitem">
Sakaguchi, Keisuke, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2020. <span>“Winogrande: An Adversarial Winograd Schema Challenge at Scale.”</span> <em>Proceedings of the AAAI Conference on Artificial Intelligence</em> 34 (05): 8732–40.
</div>
<div id="ref-talmor2018commonsenseqa" class="csl-entry" role="listitem">
Talmor, Alon, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2018. <span>“Commonsenseqa: A Question Answering Challenge Targeting Commonsense Knowledge.”</span> <em>arXiv Preprint arXiv:1811.00937</em>.
</div>
</div>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Maybe for good reason? The motivation is clear from a Titan-first perspective, perhaps less so from a Transformers-first one.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Thanks to Phil Wang and other contributors for https://github.com/lucidrains/titans-pytorch !!<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>because we were running late to formal<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>Again, these were super rushed experiments.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>